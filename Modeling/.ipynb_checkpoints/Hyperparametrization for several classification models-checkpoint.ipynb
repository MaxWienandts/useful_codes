{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f099c-f236-4e83-884e-d2924fd79f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imblearn\n",
    "from category_encoders.target_encoder import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ea115-5aa7-4d57-a929-9254eb11e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name):\n",
    "    '''\n",
    "    This function must receive a Pandas DataFrame.\n",
    "    The first columns must be the parameters tested.\n",
    "    The second column must be the results for the train dataset\n",
    "    The third column must be the results for the test dataset\n",
    "    '''\n",
    "    # Find best parameter and print\n",
    "    if maximum_minimum == 'maximum':\n",
    "        best_parameter_result = df.iloc[:,2].max()\n",
    "    elif maximum_minimum == 'minimum':\n",
    "        best_parameter_result = df.iloc[:,2].min()\n",
    "    print('Used metric:', metric_name)\n",
    "    print('Best', tested_parameter, 'parameter:', df[np.in1d(df.iloc[:,2], best_parameter_result)].iloc[:,0].values[0])\n",
    "    print('Best', tested_parameter, 'parameter result:', best_parameter_result, '\\n')\n",
    "\n",
    "    # Plot train test results\n",
    "    parameters_tested = df.iloc[:,0]\n",
    "    train_results = df.iloc[:,1]\n",
    "    test_results = df.iloc[:,2]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # Plot first the results for train\n",
    "    ax.plot(parameters_tested, train_results, color='red', linewidth = 0.7, linestyle = 'solid', label = 'Train')\n",
    "    # Add dots in tested points\n",
    "    ax.scatter(parameters_tested, train_results, s = 8, color = 'black')\n",
    "\n",
    "    # Plot the results for test\n",
    "    ax.plot(parameters_tested, test_results, color='blue', linewidth = 0.7, linestyle = 'solid', label = 'Test')\n",
    "    # Add dots in tested points\n",
    "    ax.scatter(parameters_tested, test_results, s = 8, color = 'black')\n",
    "\n",
    "\n",
    "    ax.legend(loc = 'lower center', bbox_to_anchor=(0.5, -0.18))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d498c17-f4fd-4fc1-934c-5ba301114e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target encoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "\n",
    "X_train_target_encoder = X_train.copy()\n",
    "X_test_target_encoder = X_test.copy()\n",
    "\n",
    "# target encoder for categorical varibles.\n",
    "    target_encoder_dict = {}\n",
    "    for col in df_train_bootstrap[categorical_variables]:\n",
    "        target_encoder_dict[col] = TargetEncoder().fit(X_train_target_encoder[col], y_train)\n",
    "    for key in target_encoder_dict:\n",
    "        X_train_target_encoder[key] = target_encoder_dict[key].transform(X_train_target_encoder[key])\n",
    "        X_test_target_encoder[key] = target_encoder_dict[key].transform(X_test_target_encoder[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b80da95-3dd0-4028-88a6-5a0f479288f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer([(\"onehot\", OneHotEncoder(sparse=False, drop=\"first\"), categorical_variables)], remainder='drop')\n",
    "\n",
    "one_hot_fit = ct.fit(X_train[categorical_variables])\n",
    "X_train_cat_one_hot  = one_hot_fit.transform(X_train[categorical_variables])\n",
    "X_test_cat_one_hot  = one_hot_fit.transform(X_test[categorical_variables])\n",
    "X_out_of_time_cat_one_hot = one_hot_fit.transform(X_out_of_time[categorical_variables])\n",
    "\n",
    "X_train_one_hot = pd.concat([X_train[scalar_variables], pd.DataFrame(X_train_cat_one_hot, columns=ct.get_feature_names_out())], axis=1)\n",
    "X_test_one_hot = pd.concat([X_test[scalar_variables], pd.DataFrame(X_test_cat_one_hot, columns=ct.get_feature_names_out())], axis=1)\n",
    "X_out_of_time_one_hot = pd.concat([X_out_of_time[scalar_variables], pd.DataFrame(X_out_of_time_cat_one_hot, columns=ct.get_feature_names_out())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb797a4-fa15-47c3-86f9-692b1873fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with one-hot encoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def hyperparameter_logistic(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    The choice of the algorithm depends on the penalty chosen: Supported penalties by solver:\n",
    "        ‘newton-cg’ - [‘l2’, ‘none’]\n",
    "        ‘lbfgs’ - [‘l2’, ‘none’]\n",
    "        ‘liblinear’ - [‘l1’, ‘l2’]\n",
    "        ‘sag’ - [‘l2’, ‘none’]\n",
    "        ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]\n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        lr = LogisticRegression(**hyperparameters_dict)\n",
    "\n",
    "        # fit and predict\n",
    "        lr_fit = lr.fit(X_train, y_train)\n",
    "        y_pred_train = lr_fit.predict(X_train)\n",
    "        y_pred_test = lr_fit.predict(X_test)\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Plot comparison between train and test \n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'max_iter': 100\n",
    "                        , 'solver': 'liblinear'\n",
    "                        , 'penalty': 'l1'\n",
    "                        , 'C': 1\n",
    "                        , 'l1_ratio': None}\n",
    "\n",
    "print('Test solver liblinear')\n",
    "# Test C parameter with L1\n",
    "hyperparameters_dict['penalty'] = 'l1'\n",
    "hyperparameter_test_name = 'C'\n",
    "test_hyperparameters = np.arange(start = 0.01, stop = 0.5, step = 0.05)\n",
    "hyperparameter_result = hyperparameter_logistic(X_train_one_hot, y_train[target_name], X_test_one_hot, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "\n",
    "# Test C parameter with L2\n",
    "hyperparameters_dict['penalty'] = 'l2'\n",
    "hyperparameter_test_name = 'C'\n",
    "test_hyperparameters = np.arange(start = 0.01, stop = 0.5, step = 0.05)\n",
    "hyperparameter_result = hyperparameter_logistic(X_train_one_hot, y_train[target_name], X_test_one_hot, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195f0d0-f7fc-4965-9140-b018ba1c5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nystroem + SGD\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# probability estimates are not available for loss='hinge'\n",
    "\n",
    "kwargs_Nystroem = {'kernel': 'rbf', 'random_state': 1}\n",
    "nystroem = Nystroem(**kwargs_Nystroem)\n",
    "X_train_target_encoder_nystroem = nystroem.fit_transform(X_train_target_encoder)\n",
    "X_test_target_encoder_nystroem = nystroem.fit_transform(X_test_target_encoder)\n",
    "\n",
    "def hyperparameter_SGDClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    max_iter, default=1000\n",
    "    loss{'log_loss', 'modified_huber'}\n",
    "    penalty{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’\n",
    "    l1_ratio, default=0.15. Use only if penalty = elasticnet\n",
    "    alpha, default=0.0001, Values must be in the range [0.0, inf)\n",
    "    eta0, default=0.0\n",
    "     \n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        sgd = SGDClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        sgd_fit = sgd.fit(X_train, y_train)\n",
    "        y_pred_train = sgd_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = sgd_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'max_iter': 50000\n",
    "                        , 'loss': 'log_loss'\n",
    "                        , 'learning_rate': 'optimal'\n",
    "                        , 'eta0': 0.0\n",
    "                        , 'penalty': 'l2'\n",
    "                        , 'alpha': 175\n",
    "                        , 'l1_ratio': 0.15}\n",
    "\n",
    "# Testa loss function\n",
    "print('Test loss: log_loss or modified_huber.')\n",
    "hyperparameter_test_name = 'loss'\n",
    "test_hyperparameters = ['log_loss', 'modified_huber']\n",
    "hyperparameter_result = hyperparameter_SGDClassifier(X_train_target_encoder_nystroem, y_train[target_name], X_test_target_encoder_nystroem, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# Testa penalty\n",
    "print('Test penalty: l2, l1, elasticnet.')\n",
    "hyperparameter_test_name = 'penalty'\n",
    "test_hyperparameters = ['l2', 'l1', 'elasticnet']\n",
    "hyperparameter_result = hyperparameter_SGDClassifier(X_train_target_encoder_nystroem, y_train[target_name], X_test_target_encoder_nystroem, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# Testa eta0\n",
    "print('Test eta0: 0 to 0.7.')\n",
    "hyperparameters_dict['learning_rate'] = 'constant'\n",
    "hyperparameter_test_name = 'eta0'\n",
    "test_hyperparameters = np.arange(start = 0.001, stop = 0.7, step = 0.01)\n",
    "hyperparameter_result = hyperparameter_SGDClassifier(X_train_target_encoder_nystroem, y_train[target_name], X_test_target_encoder_nystroem, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict['learning_rate'] = 'optimal'\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# Testa alpha\n",
    "print('Test alpha')\n",
    "hyperparameter_test_name = 'alpha'\n",
    "test_hyperparameters = [0.0001, 0.001, 0.01, 0.1, 1, 10, 50, 100, 125, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 250, 300]\n",
    "hyperparameter_result = hyperparameter_SGDClassifier(X_train_target_encoder_nystroem, y_train[target_name], X_test_target_encoder_nystroem, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340c583-4640-44bd-866e-8f915cd71fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Neighbors Classifier.\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "def hyperparameter_KNeighborsClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    n_neighbors: int, default = 5\n",
    "    weights: 'uniform', 'distance',  default = 'uniform'\n",
    "    algorithm{'auto', 'ball_tree', 'kd_tree', 'brute'}, default = ’auto’\n",
    "    p: 1 for l1, 2 for l2\n",
    "    metric: “euclidean”, “manhattan”, “chebyshev”, “minkowski”, defalut = ’minkowski’  (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric)\n",
    "     \n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        knn = KNeighborsClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        knn_fit = knn.fit(X_train, y_train)\n",
    "        y_pred_train = knn_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = knn_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "hyperparameters_dict = {'n_neighbors': 11\n",
    "                        , 'weights': 'distance'\n",
    "                        , 'algorithm': 'auto'\n",
    "                        , 'p': 1\n",
    "                        , 'metric': 'manhattan'}\n",
    "\n",
    "# Testa metric\n",
    "print('Test metric:  euclidean, manhattan, chebyshev, minkowski.')\n",
    "hyperparameter_test_name = 'metric'\n",
    "test_hyperparameters = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "hyperparameter_result = hyperparameter_KNeighborsClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa weights\n",
    "print('Test weights: uniform, distance.')\n",
    "hyperparameter_test_name = 'weights'\n",
    "test_hyperparameters = ['uniform', 'distance']\n",
    "hyperparameter_result = hyperparameter_KNeighborsClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa algorithm\n",
    "print('Test algorithm: auto, ball_tree, kd_tree, brute.')\n",
    "hyperparameter_test_name = 'algorithm'\n",
    "test_hyperparameters = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "hyperparameter_result = hyperparameter_KNeighborsClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa p\n",
    "print('Test p: 1 or 2.')\n",
    "hyperparameter_test_name = 'p'\n",
    "test_hyperparameters = [1, 2]\n",
    "hyperparameter_result = hyperparameter_KNeighborsClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa n_neighbors\n",
    "print('Test n_neighbors: 1 to 30.')\n",
    "hyperparameter_test_name = 'n_neighbors'\n",
    "test_hyperparameters = np.arange(start = 1, stop = 31, step = 1)\n",
    "hyperparameter_result = hyperparameter_KNeighborsClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# The best KNN model was\n",
    "'n_neighbors': 11\n",
    "'weights': 'distance'\n",
    "'algorithm': 'auto'\n",
    "'p': 1\n",
    "'metric': 'manhattan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3e4db-4bd7-4c9e-abeb-5fad09bf2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def hyperparameter_DecisionTreeClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    criterion{'gini', 'entropy', 'log_loss'}, default=”gini”\n",
    "    min_samples_split, default=2\n",
    "    min_samples_leaf, default=1\n",
    "    max_depth, default=None    \n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        dt = DecisionTreeClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        dt_fit = dt.fit(X_train, y_train)\n",
    "        y_pred_train = dt_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = dt_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'criterion': 'entropy'\n",
    "                        , 'min_samples_split': 3000\n",
    "                        , 'min_samples_leaf': 3000\n",
    "                        , 'max_depth': 10}\n",
    "\n",
    "# Testa criterion\n",
    "print('Test criterion:  gini, entropy, log_loss.')\n",
    "hyperparameter_test_name = 'criterion'\n",
    "test_hyperparameters = ['gini', 'entropy', 'log_loss']\n",
    "hyperparameter_result = hyperparameter_DecisionTreeClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_leaf\n",
    "print('Test min_samples_leaf:  3000 to 15000.')\n",
    "hyperparameter_test_name = 'min_samples_leaf'\n",
    "test_hyperparameters = [3000, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 15000]\n",
    "hyperparameter_result = hyperparameter_DecisionTreeClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_split\n",
    "print('Test min_samples_split:  3000 to 15000.')\n",
    "hyperparameter_test_name = 'min_samples_split'\n",
    "test_hyperparameters = [3000, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 15000]\n",
    "hyperparameter_result = hyperparameter_DecisionTreeClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_depth\n",
    "print('Test max_depth:  1 to 40.')\n",
    "hyperparameter_test_name = 'max_depth'\n",
    "test_hyperparameters = np.arange(start = 10, stop = 41, step = 1)\n",
    "hyperparameter_result = hyperparameter_DecisionTreeClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1f4ff-8232-4434-91bf-180aa8bab81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def hyperparameter_RandomForestClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    n_estimators, default=100. The number of trees in the forest.\n",
    "    criterion{'gini', 'entropy', 'log_loss'}, default=”gini”\n",
    "    min_samples_split, default=2\n",
    "    min_samples_leaf, default=1\n",
    "    max_depth, default=None    \n",
    "    bootstrap, default=True. \n",
    "    oob_score, default=False. Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.\n",
    "    max_samples, default=None. If int, then draw max_samples samples. If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0.0, 1.0].\n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        RF = RandomForestClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        RF_fit = RF.fit(X_train, y_train)\n",
    "        y_pred_train = RF_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = RF_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'criterion': 'entropy'\n",
    "                        , 'n_estimators': 50\n",
    "                        , 'min_samples_split': 3000\n",
    "                        , 'min_samples_leaf': 3000\n",
    "                        , 'max_depth': 10\n",
    "                        , 'oob_score': True\n",
    "                        , 'max_samples': 1}\n",
    "\n",
    "\n",
    "# Testa n_estimators\n",
    "print('Test n_estimators:  10 to 1000.')\n",
    "hyperparameter_test_name = 'n_estimators'\n",
    "test_hyperparameters = [10, 25, 50, 75, 100, 150, 200, 250, 500, 750, 1000]\n",
    "hyperparameter_result = hyperparameter_RandomForestClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa criterion\n",
    "print('Test criterion:  gini, entropy, log_loss.')\n",
    "hyperparameter_test_name = 'criterion'\n",
    "test_hyperparameters = ['gini', 'entropy', 'log_loss']\n",
    "hyperparameter_result = hyperparameter_RandomForestClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_leaf\n",
    "print('Test min_samples_leaf:  3000 to 7000.')\n",
    "hyperparameter_test_name = 'min_samples_leaf'\n",
    "test_hyperparameters = [3000, 3250, 3500, 3750, 4000, 4500, 5000, 6000, 7000]\n",
    "hyperparameter_result = hyperparameter_RandomForestClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_split\n",
    "print('Test min_samples_split:  3000 to 7000.')\n",
    "hyperparameter_test_name = 'min_samples_split'\n",
    "test_hyperparameters = [3000, 3250, 3500, 3750, 4000, 4500, 5000, 6000, 7000]\n",
    "hyperparameter_result = hyperparameter_RandomForestClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_samples\n",
    "print('Test max_samples:  0.1 to 1.')\n",
    "hyperparameter_test_name = 'max_samples'\n",
    "test_hyperparameters = np.arange(start = 0.1, stop = 1.01, step = 0.1)\n",
    "hyperparameter_result = hyperparameter_RandomForestClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_depth\n",
    "print('Test max_depth:  1 to 40.')\n",
    "hyperparameter_test_name = 'max_depth'\n",
    "test_hyperparameters = np.arange(start = 10, stop = 20, step = 1)\n",
    "hyperparameter_result = hyperparameter_RandomForestClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238351d-7957-447f-8017-b04f5f846ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def hyperparameter_ExtraTreesClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    n_estimators, default=100. The number of trees in the forest.\n",
    "    criterion{'gini', 'entropy', 'log_loss'}, default=”gini”\n",
    "    min_samples_split, default=2\n",
    "    min_samples_leaf, default=1\n",
    "    max_depth, default=None    \n",
    "    bootstrap, default=False. \n",
    "    oob_score, default=False. Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.\n",
    "    max_samples, default=None. If int, then draw max_samples samples. If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0.0, 1.0].\n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        EF = ExtraTreesClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        EF_fit = EF.fit(X_train, y_train)\n",
    "        y_pred_train = EF_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = EF_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'criterion': 'gini'\n",
    "                        , 'n_estimators': 750\n",
    "                        , 'min_samples_split': 7000\n",
    "                        , 'min_samples_leaf': 3000\n",
    "                        , 'max_depth': 10\n",
    "                        , 'bootstrap': True\n",
    "                        , 'oob_score': True\n",
    "                        , 'max_samples': 0.9}\n",
    "\n",
    "# Testa n_estimators\n",
    "print('Test n_estimators:  10 to 1000.')\n",
    "hyperparameter_test_name = 'n_estimators'\n",
    "test_hyperparameters = [100, 200, 500, 600, 650, 700, 750, 800, 850, 900, 1000]\n",
    "hyperparameter_result = hyperparameter_ExtraTreesClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa criterion\n",
    "print('Test criterion:  gini, entropy, log_loss.')\n",
    "hyperparameter_test_name = 'criterion'\n",
    "test_hyperparameters = ['gini', 'entropy', 'log_loss']\n",
    "hyperparameter_result = hyperparameter_ExtraTreesClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_leaf\n",
    "print('Test min_samples_leaf:  3000 to 7000.')\n",
    "hyperparameter_test_name = 'min_samples_leaf'\n",
    "test_hyperparameters = [3000, 3250, 3500, 3750, 4000, 4500, 5000, 6000, 7000]\n",
    "hyperparameter_result = hyperparameter_ExtraTreesClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_split\n",
    "print('Test min_samples_split:  3000 to 7000.')\n",
    "hyperparameter_test_name = 'min_samples_split'\n",
    "test_hyperparameters = [3000, 3500, 4000, 4500, 5000, 6000, 6500, 7000, 7500, 8000, 9000, 10000, 15000]\n",
    "hyperparameter_result = hyperparameter_ExtraTreesClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_samples\n",
    "print('Test max_samples:  0.1 to 1.')\n",
    "hyperparameter_test_name = 'max_samples'\n",
    "test_hyperparameters = np.arange(start = 0.1, stop = 1.01, step = 0.1)\n",
    "hyperparameter_result = hyperparameter_ExtraTreesClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_depth\n",
    "print('Test max_depth:  1 to 40.')\n",
    "hyperparameter_test_name = 'max_depth'\n",
    "test_hyperparameters = np.arange(start = 10, stop = 20, step = 1)\n",
    "hyperparameter_result = hyperparameter_ExtraTreesClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Modelo final:\n",
    "# 'random_state': 1\n",
    "# 'n_estimators': 750\n",
    "# 'criterion': 'gini'\n",
    "# 'min_samples_split': 7000\n",
    "# 'min_samples_leaf': 3000\n",
    "# 'max_depth': 10\n",
    "# 'oob_score': True\n",
    "# 'max_samples': 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37435a-0f36-4869-9512-5dbe6dd482a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def hyperparameter_GradientBoostingClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    n_estimators, default=100. The number of trees in the forest.\n",
    "    loss{'log_loss', 'exponential'}, default = 'log_loss'\n",
    "    learning_rate, default=0.1. It ranges from 0 to inf\n",
    "    subsample, default=1.0. It ranges from 0 to 1\n",
    "    criterion{'friedman_mse', 'squared_error'}, default=’friedman_mse’\n",
    "    min_samples_split, default=2\n",
    "    min_samples_leaf, default=1\n",
    "    max_depth, default = 3  \n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        GBC = GradientBoostingClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        GBC_fit = GBC.fit(X_train, y_train)\n",
    "        y_pred_train = GBC_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = GBC_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'loss': 'exponential'\n",
    "                        , 'criterion': 'friedman_mse'\n",
    "                        , 'n_estimators': 100\n",
    "                        , 'min_samples_split': 3000\n",
    "                        , 'min_samples_leaf': 3000\n",
    "                        , 'max_depth': 15\n",
    "                        , 'subsample': 1\n",
    "                        , 'learning_rate': 0.5}\n",
    "\n",
    "# Testa loss\n",
    "print('Test loss:  log_loss, exponential.')\n",
    "hyperparameter_test_name = 'loss'\n",
    "test_hyperparameters = ['log_loss', 'exponential']\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa criterion\n",
    "print('Test criterion:  friedman_mse, squared_error.')\n",
    "hyperparameter_test_name = 'criterion'\n",
    "test_hyperparameters = ['friedman_mse', 'squared_error']\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa subsample\n",
    "print('Test subsample:  0 to 1.')\n",
    "hyperparameter_test_name = 'subsample'\n",
    "test_hyperparameters = np.arange(start = 0.1, stop = 1.01, step = 0.1)\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_leaf\n",
    "print('Test min_samples_leaf:  3000 to 7000.')\n",
    "hyperparameter_test_name = 'min_samples_leaf'\n",
    "test_hyperparameters = [3000, 3500, 4000, 5000, 6000, 7000]\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa min_samples_split\n",
    "print('Test min_samples_split:  3000 to 7000.')\n",
    "hyperparameter_test_name = 'min_samples_split'\n",
    "test_hyperparameters = [3000, 3500, 4000, 5000, 6000, 7000]\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_depth\n",
    "print('Test max_depth:  1 to 20.')\n",
    "hyperparameter_test_name = 'max_depth'\n",
    "test_hyperparameters = np.arange(start = 10, stop = 20, step = 1)\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa learning_rate\n",
    "print('Test learning_rate:  0.1 to 1.')\n",
    "hyperparameter_test_name = 'learning_rate'\n",
    "test_hyperparameters = np.arange(start = 0.1, stop = 1.1, step = 0.1)\n",
    "hyperparameter_result = hyperparameter_GradientBoostingClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750dbcb-46f8-40e2-8ce4-8484b477dd6e",
   "metadata": {},
   "source": [
    "Agora, vamos tentar otimizar os modelo anteriores usando o AdaBoostClassifier. \\\n",
    "Observação que o algoritmo AdaBoostClassifier somente funciona em modelos que possuem o atributo classes_ e n_classes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac501f-72ae-4b26-b3ab-7018468b7a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def hyperparameter_AdaBoostClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    base_estimator, default = DecisionTreeClassifier initialized with max_depth=1\n",
    "    n_estimators, default = 50. It ranges from 1 to inf.  \n",
    "    learning_rate, default = 1.0. It ranges from 0 to inf.\n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        ABC = AdaBoostClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        ABC_fit = ABC.fit(X_train, y_train)\n",
    "        y_pred_train = ABC_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = ABC_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "DecisionTreeClassifier_hyperparameters_dict = {'random_state': 1\n",
    "                                                , 'criterion': 'entropy'\n",
    "                                                , 'min_samples_split': 3000\n",
    "                                                , 'min_samples_leaf': 3000\n",
    "                                                , 'max_depth': 10}\n",
    "\n",
    "# RandomForestClassifier does not work well with AdaBoost.\n",
    "# RandomForestClassifier_hyperparameters_dict = {'random_state': 1\n",
    "#                         , 'criterion': 'entropy'\n",
    "#                         , 'n_estimators': 50\n",
    "#                         , 'min_samples_split': 3000\n",
    "#                         , 'min_samples_leaf': 3000\n",
    "#                         , 'max_depth': 10\n",
    "#                         , 'oob_score': True\n",
    "#                         , 'max_samples': 1}\n",
    "\n",
    "# Demora muito testar o AdaBoost com ExtraTreesClassifier\n",
    "# ExtraTreesClassifier_hyperparameters_dict = {'random_state': 1\n",
    "#                         , 'criterion': 'gini'\n",
    "#                         , 'n_estimators': 750\n",
    "#                         , 'min_samples_split': 7000\n",
    "#                         , 'min_samples_leaf': 3000\n",
    "#                         , 'max_depth': 10\n",
    "#                         , 'bootstrap': True\n",
    "#                         , 'oob_score': True\n",
    "#                         , 'max_samples': 0.9}\n",
    "\n",
    "hyperparameters_dict = {'random_state': 1\n",
    "                        , 'base_estimator': DecisionTreeClassifier(**DecisionTreeClassifier_hyperparameters_dict)\n",
    "                        , 'n_estimators': 50       # O ganho em aumentar esse hiperparâmetro não é muito e aumenta muito o overit.\n",
    "                        , 'learning_rate': 0.3}\n",
    "\n",
    "# # Testa n_estimators para DecisionTreeClassifier\n",
    "# print('Test n_estimators for DecisionTreeClassifier:  10 to 200.')\n",
    "# hyperparameter_test_name = 'n_estimators'\n",
    "# test_hyperparameters = [10, 50, 100, 200]\n",
    "# hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # Plota resultados\n",
    "# df = hyperparameter_result['resultados'].copy()\n",
    "# tested_parameter = hyperparameter_test_name\n",
    "# maximum_minimum = 'maximum'\n",
    "# metric_name = 'ROC AUC'\n",
    "# print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # Salva melhor resultado.\n",
    "# hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# Testa learning_rate para DecisionTreeClassifier. ROC sem AdaBoost: 0.8374951583700451, com AdaBoost: 0.8666174044250164\n",
    "print('Test learning_rate for DecisionTreeClassifier:  0.1 to 1.')\n",
    "hyperparameter_test_name = 'learning_rate'\n",
    "test_hyperparameters = [0.1, 0.2, 0.3, 0.5, 0.8, 1]\n",
    "hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "\n",
    "# # Testa n_estimators para RandomForestClassifier\n",
    "# hyperparameters_dict['base_estimator'] = RandomForestClassifier(**RandomForestClassifier_hyperparameters_dict)\n",
    "# print('Test n_estimators for RandomForestClassifier:  10 to 200.')\n",
    "# hyperparameter_test_name = 'n_estimators'\n",
    "# test_hyperparameters = [10, 50, 100, 200]\n",
    "# hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # Plota resultados\n",
    "# df = hyperparameter_result['resultados'].copy()\n",
    "# tested_parameter = hyperparameter_test_name\n",
    "# maximum_minimum = 'maximum'\n",
    "# metric_name = 'ROC AUC'\n",
    "# print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # Salva melhor resultado.\n",
    "# hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# # Testa learning_rate para RandomForestClassifier. ROC sem AdaBoost: 0.8412149997416809, com AdaBoost: 0.5\n",
    "# print('Test learning_rate for RandomForestClassifier:  0.1 to 1.')\n",
    "# hyperparameters_dict['base_estimator'] = RandomForestClassifier(**RandomForestClassifier_hyperparameters_dict)\n",
    "# hyperparameter_test_name = 'learning_rate'\n",
    "# test_hyperparameters = [0.1, 0.2, 0.3, 0.5, 0.8, 1]\n",
    "# hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # Plota resultados\n",
    "# df = hyperparameter_result['resultados'].copy()\n",
    "# tested_parameter = hyperparameter_test_name\n",
    "# maximum_minimum = 'maximum'\n",
    "# metric_name = 'ROC AUC'\n",
    "# print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # Salva melhor resultado.\n",
    "# hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "\n",
    "# Demora muito testar o AdaBoost com ExtraTreesClassifier\n",
    "# # Testa n_estimators para ExtraTreesClassifier\n",
    "# hyperparameters_dict['base_estimator'] = ExtraTreesClassifier(**ExtraTreesClassifier_hyperparameters_dict)\n",
    "# print('Test n_estimators for ExtraTreesClassifier:  10 to 200.')\n",
    "# hyperparameter_test_name = 'n_estimators'\n",
    "# test_hyperparameters = [10, 50, 100, 200]\n",
    "# hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # Plota resultados\n",
    "# df = hyperparameter_result['resultados'].copy()\n",
    "# tested_parameter = hyperparameter_test_name\n",
    "# maximum_minimum = 'maximum'\n",
    "# metric_name = 'ROC AUC'\n",
    "# print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # Salva melhor resultado.\n",
    "# hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# # Testa learning_rate para ExtraTreesClassifier. ROC sem AdaBoost: 0.8195196098946576, com AdaBoost:  Demora muito!!\n",
    "# hyperparameters_dict['base_estimator'] = ExtraTreesClassifier(**ExtraTreesClassifier_hyperparameters_dict)\n",
    "# print('Test learning_rate for ExtraTreesClassifier:  0.1 to 1.')\n",
    "# hyperparameter_test_name = 'learning_rate'\n",
    "# test_hyperparameters = [0.1, 0.2, 0.3, 0.5, 0.8, 1]\n",
    "# hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # Plota resultados\n",
    "# df = hyperparameter_result['resultados'].copy()\n",
    "# tested_parameter = hyperparameter_test_name\n",
    "# maximum_minimum = 'maximum'\n",
    "# metric_name = 'ROC AUC'\n",
    "# print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # Salva melhor resultado.\n",
    "# hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ca91b-3982-4d04-9db6-07ef9804b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GradientBoostingClassifier com AdaBoost - Demora mais de 2:30 horas para correr.\n",
    "# GradientBoostingClassifier_hyperparameters_dict = {'random_state': 1\n",
    "#                         , 'loss': 'exponential'\n",
    "#                         , 'criterion': 'friedman_mse'\n",
    "#                         , 'n_estimators': 100\n",
    "#                         , 'min_samples_split': 3000\n",
    "#                         , 'min_samples_leaf': 3000\n",
    "#                         , 'max_depth': 15\n",
    "#                         , 'subsample': 1\n",
    "#                         , 'learning_rate': 0.5}\n",
    "\n",
    "\n",
    "# hyperparameters_dict = {'random_state': 1\n",
    "#                         , 'base_estimator': GradientBoostingClassifier(**GradientBoostingClassifier_hyperparameters_dict)\n",
    "#                         , 'n_estimators': 50       # O ganho em aumentar esse hiperparâmetro não é muito e aumenta muito o overit.\n",
    "#                         , 'learning_rate': 0.3}\n",
    "\n",
    "# # # Testa n_estimators para GradientBoostingClassifier\n",
    "# # hyperparameters_dict['base_estimator'] = GradientBoostingClassifier(**GradientBoostingClassifier_hyperparameters_dict)\n",
    "# # print('Test n_estimators for GradientBoostingClassifier:  10 to 200.')\n",
    "# # hyperparameter_test_name = 'n_estimators'\n",
    "# # test_hyperparameters = [10, 50, 100, 200]\n",
    "# # hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # # Plota resultados\n",
    "# # df = hyperparameter_result['resultados'].copy()\n",
    "# # tested_parameter = hyperparameter_test_name\n",
    "# # maximum_minimum = 'maximum'\n",
    "# # metric_name = 'ROC AUC'\n",
    "# # print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # # Salva melhor resultado.\n",
    "# # hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "# # Testa learning_rate para GradientBoostingClassifier. ROC sem AdaBoost: 0.8722012787817987, com AdaBoost:\n",
    "# hyperparameters_dict['base_estimator'] = GradientBoostingClassifier(**GradientBoostingClassifier_hyperparameters_dict)\n",
    "# print('Test learning_rate for GradientBoostingClassifier:  0.1 to 1.')\n",
    "# hyperparameter_test_name = 'learning_rate'\n",
    "# test_hyperparameters = [0.1, 0.2, 0.3, 0.5, 0.8, 1]\n",
    "# hyperparameter_result = hyperparameter_AdaBoostClassifier(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# # Plota resultados\n",
    "# df = hyperparameter_result['resultados'].copy()\n",
    "# tested_parameter = hyperparameter_test_name\n",
    "# maximum_minimum = 'maximum'\n",
    "# metric_name = 'ROC AUC'\n",
    "# print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# # Salva melhor resultado.\n",
    "# hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f90385-eed4-40b7-ba24-7e608ad4ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "def hyperparameter_LGBMClassifier(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    random_state\n",
    "    n_estimators, default=100. The number of trees in the forest.\n",
    "    loss{'log_loss', 'exponential'}, default = 'log_loss'\n",
    "    learning_rate, default=0.1. It ranges from 0 to inf\n",
    "    subsample, default=1.0. It ranges from 0 to 1\n",
    "    criterion{'friedman_mse', 'squared_error'}, default=’friedman_mse’\n",
    "    min_samples_split, default=2\n",
    "    min_samples_leaf, default=1\n",
    "    max_depth, default = 3  \n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        clf = lgb.LGBMClassifier(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        clf_fit = clf.fit(X_train, y_train)\n",
    "        y_pred_train = clf_fit.predict_proba(X_train)[:,1]\n",
    "        y_pred_test = clf_fit.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "hyperparameters_dict = {\"objective\": \"binary\",\n",
    "            \"metric\": \"binary_logloss\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            'verbosity': -1,\n",
    "            'seed': 1,\n",
    "            \"min_data_in_leaf\": 3500,\n",
    "            \"num_leaves\": 29,\n",
    "            \"max_depth\": 16,\n",
    "            \"feature_fraction\": 1,\n",
    "            \"learning_rate\": 0.43}  \n",
    "\n",
    "\n",
    "# Testa min_data_in_leaf\n",
    "print('Test min_data_in_leaf:  from 3500 to 7000.')\n",
    "hyperparameter_test_name = 'min_data_in_leaf'\n",
    "test_hyperparameters = [3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 5000, 6000, 7000]\n",
    "hyperparameter_result = hyperparameter_LGBMClassifier(X_train, y_train[target_name], X_test, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "                        \n",
    "                        \n",
    "# Testa num_leaves\n",
    "print('Test num_leaves:  from 10 to 40.')\n",
    "hyperparameter_test_name = 'num_leaves'\n",
    "test_hyperparameters = np.arange(start = 10, stop = 40, step = 1)\n",
    "hyperparameter_result = hyperparameter_LGBMClassifier(X_train, y_train[target_name], X_test, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "                        \n",
    "                        \n",
    "# Testa max_depth\n",
    "print('Test max_depth:  from 2 to 30.')\n",
    "hyperparameter_test_name = 'max_depth'\n",
    "test_hyperparameters = np.arange(start = 2, stop = 31, step = 1)\n",
    "hyperparameter_result = hyperparameter_LGBMClassifier(X_train, y_train[target_name], X_test, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "                        \n",
    "                        \n",
    "# Testa learning_rate\n",
    "print('Test learning_rate:  from 0.1 to 0.7.')\n",
    "hyperparameter_test_name = 'learning_rate'\n",
    "test_hyperparameters = np.arange(start = 0.01, stop = 0.701, step = 0.01)\n",
    "hyperparameter_result = hyperparameter_LGBMClassifier(X_train, y_train[target_name], X_test, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5bc50-8261-4279-9145-855a710c44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "def hyperparameter_XGBRegressor(X_train, y_train, X_test, y_test, hyperparameters_dict, hyperparameter_test_name, test_hyperparameters):\n",
    "    '''\n",
    "    hiperparametros:\n",
    "    'max_depth' = 6\n",
    "    'max_leaves' = 62\n",
    "    'min_child_weight' = 1\n",
    "    'subsample' = 0.95       # De 0 a 1\n",
    "    'colsample_bytree' = 1   # De 0 a 1\n",
    "    'learning_rate' = 0.3\n",
    "    'gamma' = 0.365\n",
    "    'reg_lambda' = 0\n",
    "    'reg_alpha' = 0\n",
    "    '''\n",
    "    returned_values = {}\n",
    "    metrica_comparacao_train = []\n",
    "    metrica_comparacao_test = []\n",
    "    parameters_tested = []\n",
    "    for parameter in test_hyperparameters:\n",
    "        parameters_tested.append(parameter)\n",
    "        # Muda o valor do hyperparametro testado\n",
    "        hyperparameters_dict[hyperparameter_test_name] = parameter\n",
    "        # Set model hyperparameters\n",
    "        XGBoost = xgb.XGBRegressor(**hyperparameters_dict)\n",
    "        \n",
    "        # fit and predict\n",
    "        XGBoost_fit = XGBoost.fit(X_train, y_train)\n",
    "        y_pred_train = XGBoost_fit.predict(X_train)\n",
    "        y_pred_test = XGBoost_fit.predict(X_test)\n",
    "        \n",
    "        # Guarda métricas de comparação\n",
    "        metrica_comparacao_train.append(roc_auc_score(y_train, y_pred_train))\n",
    "        metrica_comparacao_test.append(roc_auc_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Save results\n",
    "    metrica_comparacao_df = pd.DataFrame(list(zip(parameters_tested, metrica_comparacao_train, metrica_comparacao_test)), columns = ['parameter', 'roc_auc_score_train', 'roc_auc_score_test'])\n",
    "    \n",
    "    # Return the best hyperparameter and table with metrics\n",
    "    returned_values[hyperparameter_test_name] = metrica_comparacao_df[np.in1d(metrica_comparacao_df['roc_auc_score_test'], metrica_comparacao_df['roc_auc_score_test'].max())]['parameter'].values[0]\n",
    "    returned_values['resultados'] = metrica_comparacao_df\n",
    "    return returned_values\n",
    "\n",
    "hyperparameters_dict = {'objective': 'binary:logistic',\n",
    "                        'eval_metric': 'auc',\n",
    "                        'seed': 1,\n",
    "                        'verbosity': 1,\n",
    "                        'validate_parameters': True,\n",
    "                        'tree_method': \"hist\",    \n",
    "                        'booster': 'gbtree',       # gbtree, gblinear or dart.\n",
    "                        'max_depth': 6,\n",
    "                        'max_leaves': 62,\n",
    "                        'min_child_weight': 1,\n",
    "                        'subsample': 0.95,\n",
    "                        'colsample_bytree': 1,\n",
    "                        'learning_rate': 0.3,\n",
    "                        'gamma': 0.365,\n",
    "                        'reg_lambda': 0,\n",
    "                        'reg_alpha': 0}  \n",
    "\n",
    "\n",
    "# Testa min_child_weight\n",
    "print('Test min_child_weight:  from 1 to 3000.')\n",
    "hyperparameter_test_name = 'min_child_weight'\n",
    "test_hyperparameters = [1, 50, 100, 200, 400, 600, 800, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000]\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_depth\n",
    "print('Test max_depth:  from 1 to 15.')\n",
    "hyperparameter_test_name = 'max_depth'\n",
    "test_hyperparameters = np.arange(start = 1, stop = 16, step = 1)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa max_leaves\n",
    "print('Test max_leaves:  from 1 to 70.')\n",
    "hyperparameter_test_name = 'max_leaves'\n",
    "test_hyperparameters = np.arange(start = 1, stop = 71, step = 1)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa subsample\n",
    "print('Test subsample:  from 0.4 to 1.')\n",
    "hyperparameter_test_name = 'subsample'\n",
    "test_hyperparameters = np.arange(start = 0.4, stop = 1.04, step = 0.05)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa colsample_bytree\n",
    "print('Test colsample_bytree:  from 0.4 to 1.')\n",
    "hyperparameter_test_name = 'colsample_bytree'\n",
    "test_hyperparameters = np.arange(start = 0.4, stop = 1.04, step = 0.05)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa learning_rate\n",
    "print('Test learning_rate:  from 0.01 to 0.5.')\n",
    "hyperparameter_test_name = 'learning_rate'\n",
    "test_hyperparameters = np.arange(start = 0.01, stop = 0.501, step = 0.01)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa gamma\n",
    "print('Test gamma:  from 0 to 0.5.')\n",
    "hyperparameter_test_name = 'gamma'\n",
    "test_hyperparameters = np.arange(start = 0, stop = 0.5001, step = 0.005)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa reg_lambda\n",
    "print('Test reg_lambda:  from 0 to 200.')\n",
    "hyperparameter_test_name = 'reg_lambda'\n",
    "test_hyperparameters = np.arange(start = 0, stop = 201, step = 5)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n",
    "\n",
    "# Testa reg_alpha\n",
    "print('Test reg_alpha:  from 0 to 200.')\n",
    "hyperparameter_test_name = 'reg_alpha'\n",
    "test_hyperparameters = np.arange(start = 0, stop = 201, step = 5)\n",
    "hyperparameter_result = hyperparameter_XGBRegressor(X_train_target_encoder, y_train[target_name], X_test_target_encoder, y_test[target_name], hyperparameters_dict, hyperparameter_test_name, test_hyperparameters)\n",
    "# Plota resultados\n",
    "df = hyperparameter_result['resultados'].copy()\n",
    "tested_parameter = hyperparameter_test_name\n",
    "maximum_minimum = 'maximum'\n",
    "metric_name = 'ROC AUC'\n",
    "print_plot_best_hyperparameter_result(df, maximum_minimum, tested_parameter, metric_name)\n",
    "# Salva melhor resultado.\n",
    "hyperparameters_dict[hyperparameter_test_name] = hyperparameter_result[hyperparameter_test_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f5f7e-6267-4aac-91ac-6cb39c1ef796",
   "metadata": {},
   "source": [
    "Agora, vamos juntar os melhores modelos e adicionar a soft Voting Classifier.\\\n",
    "Primeiro, como nós usamos encoders diferentes em modelos diferentes, vamos recriar nossas tabelas de treino e teste e montar uma pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb35df-d9a0-4bdf-8299-f2acfa8495c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lembra as variáveis\n",
    "# Define as variáveis contínuas\n",
    "scalar_variables = ['col_1'\n",
    "                    , 'col_2']\n",
    "\n",
    "# Define as variáveis categóricas\n",
    "categorical_variables = ['col_3', 'col_4']\n",
    "\n",
    "# Define treino, teste e out of time\n",
    "X_train = df_train[X_features].copy()\n",
    "X_test = df_test[X_features].copy()\n",
    "# Target\n",
    "y_train = df_train[[target_name]].copy()\n",
    "y_test = df_test[[target_name]].copy()\n",
    "# Out of time\n",
    "X_out_of_time = df_out_of_time[X_features].copy()\n",
    "# Target\n",
    "y_out_of_time = df_out_of_time[[target_name]].copy()\n",
    "\n",
    "# Importante, na pipeline de preprocessamento é necessário que as colunas usadas em cada etapa fiquem juntas.\n",
    "# Caso contraio, as colunas não usadas serão excluídas.\n",
    "X_train = X_train[scalar_variables + categorical_variables]\n",
    "X_test = X_test[scalar_variables + categorical_variables]\n",
    "X_out_of_time = X_out_of_time[scalar_variables + categorical_variables]\n",
    "\n",
    "# Ordinal encoder used in LightGBM\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imblearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders.target_encoder import OrdinalEncoder\n",
    "pre_processamento_ordinal_encoder = ColumnTransformer([('num', StandardScaler(), scalar_variables), ('ordinal', OrdinalEncoder(), categorical_variables)])\n",
    "\n",
    "# one-hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "pre_processamento_onehot_encoder = ColumnTransformer([('num', StandardScaler(), scalar_variables), ('onehot', OneHotEncoder(sparse=False, drop=\"first\"), categorical_variables)])\n",
    "\n",
    "# target encoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "pre_processamento_target_encoder = ColumnTransformer([('num', StandardScaler(), scalar_variables), ('target', TargetEncoder(), categorical_variables)])\n",
    "\n",
    "\n",
    "# modelos com one-hot encoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "linear_model_hyperparameters_dict = {'random_state': 1 \n",
    "                        , 'max_iter': 100 \n",
    "                        , 'solver': 'liblinear' \n",
    "                        , 'penalty': 'l1' \n",
    "                        , 'C': 1 \n",
    "                        , 'l1_ratio': None}\n",
    "lr = LogisticRegression(**linear_model_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "linear_model_imba_pipeline = make_pipeline_imblearn(pre_processamento_onehot_encoder, lr)  # ROC = 0.\n",
    "# Fit model\n",
    "linear_model_fit = linear_model_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "\n",
    "# modelos com target encoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_hyperparameters_dict = {'n_neighbors': 11 \n",
    "                        , 'weights': 'distance' \n",
    "                        , 'algorithm': 'auto' \n",
    "                        , 'p': 1 \n",
    "                        , 'metric': 'manhattan'} \n",
    "knn = KNeighborsClassifier(**knn_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "knn_imba_pipeline = make_pipeline_imblearn(pre_processamento_target_encoder, knn)  # ROC = 0.\n",
    "# Fit model\n",
    "knn_fit = knn_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "DecisionTreeClassifier_hyperparameters_dict = {'random_state': 1 \n",
    "                        , 'criterion': 'entropy' \n",
    "                        , 'min_samples_split': 3000 \n",
    "                        , 'min_samples_leaf': 3000 \n",
    "                        , 'max_depth': 10} \n",
    "AdaBoost_hyperparameters_dict = {'random_state': 1 \n",
    "                        , 'base_estimator': DecisionTreeClassifier(**DecisionTreeClassifier_hyperparameters_dict) \n",
    "                        , 'n_estimators': 50    \n",
    "                        , 'learning_rate': 0.3} \n",
    "ABC_DecisionTreeClassifier = AdaBoostClassifier(**AdaBoost_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "ABC_DecisionTreeClassifier_imba_pipeline = make_pipeline_imblearn(pre_processamento_target_encoder, ABC_DecisionTreeClassifier)  # ROC = 0.8666174044250164\n",
    "# Fit model\n",
    "ABC_DecisionTreeClassifier_fit = ABC_DecisionTreeClassifier_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier_hyperparameters_dict = {'random_state': 1 \n",
    "                        , 'criterion': 'entropy' \n",
    "                        , 'n_estimators': 50 \n",
    "                        , 'min_samples_split': 3000 \n",
    "                        , 'min_samples_leaf': 3000 \n",
    "                        , 'max_depth': 10 \n",
    "                        , 'oob_score': True \n",
    "                        , 'max_samples': 1} \n",
    "RF = RandomForestClassifier(**RandomForestClassifier_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "RF_imba_pipeline = make_pipeline_imblearn(pre_processamento_target_encoder, RF)  # ROC = 0.\n",
    "# Fit model\n",
    "RF_fit = RF_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ExtraTreesClassifier_hyperparameters_dict = {'random_state': 1 \n",
    "                        , 'criterion': 'gini' \n",
    "                        , 'n_estimators': 750 \n",
    "                        , 'min_samples_split': 7000 \n",
    "                        , 'min_samples_leaf': 3000 \n",
    "                        , 'max_depth': 10 \n",
    "                        , 'bootstrap': True \n",
    "                        , 'oob_score': True \n",
    "                        , 'max_samples': 0.9} \n",
    "EF = ExtraTreesClassifier(**ExtraTreesClassifier_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "EF_imba_pipeline = make_pipeline_imblearn(pre_processamento_target_encoder, EF)  # ROC = 0.\n",
    "# Fit model\n",
    "EF_fit = EF_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GradientBoostingClassifier_hyperparameters_dict = {'random_state': 1 \n",
    "                        , 'loss': 'exponential' \n",
    "                        , 'criterion': 'friedman_mse' \n",
    "                        , 'n_estimators': 100 \n",
    "                        , 'min_samples_split': 3000 \n",
    "                        , 'min_samples_leaf': 3000 \n",
    "                        , 'max_depth': 15 \n",
    "                        , 'subsample': 1 \n",
    "                        , 'learning_rate': 0.5}\n",
    "GBC = GradientBoostingClassifier(**GradientBoostingClassifier_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "GBC_imba_pipeline = make_pipeline_imblearn(pre_processamento_target_encoder, GBC)  # ROC = 0.\n",
    "# Fit model\n",
    "GBC_fit = GBC_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb_hyperparameters_dict = { 'objective': 'binary:logistic', \n",
    "                            'use_label_encoder': False,             # Necessary only when using the scikit-learn api.\n",
    "                            'eval_metric': 'auc', \n",
    "                            'seed': 1, \n",
    "                            'verbosity': 1, \n",
    "                            'validate_parameters': True, \n",
    "                            'tree_method': \"hist\",     \n",
    "                            'booster': 'gbtree',       # gbtree, gblinear or dart. \\\n",
    "                            'max_depth': 6, \n",
    "                            'max_leaves': 62, \n",
    "                            'min_child_weight': 1, \n",
    "                            'subsample': 0.95, \n",
    "                            'colsample_bytree': 1, \n",
    "                            'learning_rate': 0.3, \n",
    "                            'gamma': 0.365, \n",
    "                            'reg_lambda': 0, \n",
    "                            'reg_alpha': 0}\n",
    "# XGBoost = xgb.XGBRegressor(**xgb_hyperparameters_dict)\n",
    "XGBoost = xgb.XGBClassifier(**xgb_hyperparameters_dict)          # We must use scikit-learn api to use the predict_proba method in the soft vote model.\n",
    "# Criando pipeline com o modelo\n",
    "XGBoost_imba_pipeline = make_pipeline_imblearn(pre_processamento_target_encoder, XGBoost)  # ROC = 0.\n",
    "# Fit model\n",
    "XGBoost_fit = XGBoost_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "\n",
    "\n",
    "# Modelos com ordinal encoder\n",
    "import lightgbm as lgb\n",
    "lightGBM_hyperparameters_dict = {\"objective\": \"binary\", \n",
    "            \"metric\": \"binary_logloss\", \n",
    "            \"boosting_type\": \"gbdt\", \n",
    "            'verbosity': -1, \n",
    "            'seed': 1, \n",
    "            \"min_data_in_leaf\": 3500, \n",
    "            \"num_leaves\": 29, \n",
    "            \"max_depth\": 16, \n",
    "            \"feature_fraction\": 1, \n",
    "            \"learning_rate\": 0.43}\n",
    "lightGBM = lgb.LGBMClassifier(**lightGBM_hyperparameters_dict)\n",
    "# Criando pipeline com o modelo\n",
    "lightGBM_imba_pipeline = make_pipeline_imblearn(pre_processamento_ordinal_encoder, lightGBM) # ROC = 0.\n",
    "# Fit model\n",
    "lightGBM_fit = lightGBM_imba_pipeline.fit(X_train, y_train['FLAG_TARGET_'], lgbmclassifier__categorical_feature = [14, 15, 16, 17])\n",
    "\n",
    "\n",
    "# Soft voting model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "estimators = [('linear_model_fit', linear_model_fit)    # ROC = 0.\n",
    "              , ('knn_fit', knn_fit)                    # ROC = 0. \n",
    "              , ('ABC_DecisionTreeClassifier_fit', ABC_DecisionTreeClassifier_fit)   # ROC = 0.\n",
    "              , ('RF_fit', RF_fit)       # ROC = 0.\n",
    "              , ('EF_fit', EF_fit)       # ROC = 0.\n",
    "              , ('GBC_fit', GBC_fit)     # ROC = 0.0.\n",
    "              , ('XGBoost_fit', XGBoost_fit)   # ROC = 0.0.\n",
    "              , ('lightGBM_fit', lightGBM_fit)]   # ROC = 0. \n",
    "\n",
    "VC = VotingClassifier(estimators, voting='soft')\n",
    "VC = VC.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "# Predict in our test.\n",
    "y_pred_test = VC.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test['FLAG_TARGET_'], y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e100eab-32cd-4b93-bc89-70720e89a97f",
   "metadata": {},
   "source": [
    "Vamos tentar tirar alguns estimadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a5b2d-dff7-42ea-9d4c-85827e6e582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [#('linear_model_fit', linear_model_fit)    # ROC = 0.\n",
    "               ('knn_fit', knn_fit)                    # ROC = 0. \n",
    "              , ('ABC_DecisionTreeClassifier_fit', ABC_DecisionTreeClassifier_fit)   # ROC = 0.\n",
    "              , ('RF_fit', RF_fit)       # ROC = 0.\n",
    "              , ('EF_fit', EF_fit)       # ROC = 0.\n",
    "              , ('GBC_fit', GBC_fit)     # ROC = 0.0.\n",
    "              , ('XGBoost_fit', XGBoost_fit)   # ROC = 0.0.\n",
    "              , ('lightGBM_fit', lightGBM_fit)]   # ROC = 0. \n",
    "\n",
    "VC = VotingClassifier(estimators, voting='soft')\n",
    "VC = VC.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "# Predict in our test.\n",
    "y_pred_test = VC.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test['FLAG_TARGET_'], y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ea2f5-24d4-43e2-af65-994d07b21f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [#('linear_model_fit', linear_model_fit)    # ROC = 0.\n",
    "               ('knn_fit', knn_fit)                    # ROC = 0. \n",
    "              , ('ABC_DecisionTreeClassifier_fit', ABC_DecisionTreeClassifier_fit)   # ROC = 0.\n",
    "              , ('RF_fit', RF_fit)       # ROC = 0.\n",
    "              # , ('EF_fit', EF_fit)       # ROC = 0.\n",
    "              , ('GBC_fit', GBC_fit)     # ROC = 0.0.\n",
    "              , ('XGBoost_fit', XGBoost_fit)   # ROC = 0.0.\n",
    "              , ('lightGBM_fit', lightGBM_fit)]   # ROC = 0. \n",
    "\n",
    "VC = VotingClassifier(estimators, voting='soft')\n",
    "VC = VC.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "# Predict in our test.\n",
    "y_pred_test = VC.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test['FLAG_TARGET_'], y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d20716-842e-47ed-9869-b1c640f9d1c8",
   "metadata": {},
   "source": [
    "Ao invés de usar um soft voting, vamos tentar o StackingClassifier com uma regressão logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2268f76-5ff3-47cf-9910-88d10b588a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "estimators = [('linear_model_fit', linear_model_fit)    # ROC = 0.\n",
    "              , ('knn_fit', knn_fit)                    # ROC = 0. \n",
    "              , ('ABC_DecisionTreeClassifier_fit', ABC_DecisionTreeClassifier_fit)   # ROC = 0.\n",
    "              , ('RF_fit', RF_fit)       # ROC = 0.\n",
    "              , ('EF_fit', EF_fit)       # ROC = 0.\n",
    "              , ('GBC_fit', GBC_fit)     # ROC = 0.0.\n",
    "              , ('XGBoost_fit', XGBoost_fit)   # ROC = 0.0.\n",
    "              , ('lightGBM_fit', lightGBM_fit)]   # ROC = 0. \n",
    "\n",
    "SC = StackingClassifier(estimators = estimators, final_estimator=LogisticRegression())\n",
    "SC_fit = SC.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "# Predict in our test.\n",
    "y_pred_test = SC_fit.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test['FLAG_TARGET_'], y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a4a57-0fa8-4c62-a402-ab9d4c2fbd77",
   "metadata": {},
   "source": [
    "Vamos tentar tirar alguns estimadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557a424-1974-4fee-895c-a7b315395d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [#('linear_model_fit', linear_model_fit)    # ROC = 0.\n",
    "               ('knn_fit', knn_fit)                    # ROC = 0. \n",
    "              , ('ABC_DecisionTreeClassifier_fit', ABC_DecisionTreeClassifier_fit)   # ROC = 0.\n",
    "              , ('RF_fit', RF_fit)       # ROC = 0.\n",
    "              , ('EF_fit', EF_fit)       # ROC = 0.\n",
    "              , ('GBC_fit', GBC_fit)     # ROC = 0.0.\n",
    "              , ('XGBoost_fit', XGBoost_fit)   # ROC = 0.0.\n",
    "              , ('lightGBM_fit', lightGBM_fit)]   # ROC = 0. \n",
    "\n",
    "SC = StackingClassifier(estimators = estimators, final_estimator=LogisticRegression())\n",
    "SC_fit = SC.fit(X_train, y_train['FLAG_TARGET_'])\n",
    "# Predict in our test.\n",
    "y_pred_test = SC_fit.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test['FLAG_TARGET_'], y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
